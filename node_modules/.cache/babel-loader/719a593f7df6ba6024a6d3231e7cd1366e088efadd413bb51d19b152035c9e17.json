{"ast":null,"code":"export const mediumTexts = [{\n  id: 1,\n  title: \"The Neuroplasticity of the Human Brain\",\n  text: \"Neuroplasticity refers to the brain's remarkable ability to reorganize itself by forming new neural connections throughout life. This phenomenon allows the neurons in the brain to compensate for injury and disease and to adjust their activities in response to new situations or changes in their environment. It was once believed that the brain's neural networks were largely fixed by early adulthood, with minimal capacity for adaptation or change. Modern neuroscientific research has completely upended this perspective, demonstrating that the brain maintains an extraordinary degree of malleability even into advanced age. Neuroplasticity operates on multiple levels, from microscopic changes in individual neurons to large-scale cortical remapping. The term encompasses both structural plasticity, which refers to changes in the physical structure of the brain, and functional plasticity, which refers to the brain's ability to move functions from damaged areas to undamaged ones. At the cellular level, neuroplasticity manifests through mechanisms such as axonal sprouting, dendritic remodeling, and synaptogenesis. Axonal sprouting occurs when undamaged axons grow new nerve endings to reconnect with neurons that have lost their connections. Dendritic remodeling involves changes in the number and distribution of dendritic spines, which are small protrusions that receive input from a single synapse of an axon. Synaptogenesis refers to the formation of new synaptic connections between neurons, facilitated by the production of various neurotrophic factors. These microscopic changes collectively enhance the brain's ability to process and encode information, supporting learning, memory, and recovery from injury. Research has identified several key drivers of neuroplastic change. Environmental enrichment, characterized by novel, complex, and challenging experiences, has been shown to significantly enhance neural development and plasticity. Physical exercise promotes the release of growth factors such as brain-derived neurotrophic factor (BDNF), which supports neuronal survival and growth. Cognitive stimulation through learning new skills or solving complex problems activates neuroplastic mechanisms, strengthening existing neural pathways and creating new ones. Conversely, stress, social isolation, and certain neurotoxins can inhibit neuroplasticity, highlighting the importance of psychosocial factors in brain health. Neuroplasticity has profound implications for rehabilitation following brain injury or stroke. Constraint-induced movement therapy, for example, forces the use of affected limbs by restraining unaffected ones, thereby promoting cortical reorganization and functional recovery. Similarly, cognitive rehabilitation strategies leverage neuroplastic mechanisms to help individuals recover from traumatic brain injuries or neurodegenerative conditions. Some therapeutic approaches aim to harness neuroplasticity through neurofeedback, where individuals learn to modulate their brain activity through real-time feedback, or through brain stimulation techniques that directly influence neural activity. The field of neural engineering has made significant advances in developing brain-computer interfaces that can potentially restore sensory and motor functions by establishing direct communication pathways between the brain and external devices. These technologies rely on the brain's neuroplastic capacity to interpret and adapt to novel inputs. Understanding neuroplasticity has also revolutionized educational approaches. Recognition of sensitive periods in development, during which the brain is particularly receptive to specific types of stimulation, has informed early childhood education strategies. Moreover, the knowledge that learning continues throughout life has validated the importance of lifelong education and cognitive engagement for maintaining brain health into old age. Despite these advances, many questions about neuroplasticity remain unanswered. Researchers continue to investigate the molecular mechanisms that regulate different forms of plasticity, seeking to develop pharmacological interventions that could enhance beneficial neuroplastic changes or inhibit maladaptive ones. The interaction between genetic factors and environmental influences in shaping neuroplastic responses represents another important area of ongoing research.\"\n}, {\n  id: 2,\n  title: \"The Economic Implications of Artificial Intelligence\",\n  text: \"Artificial intelligence (AI) represents one of the most transformative technological forces of our time, with far-reaching economic implications that extend across virtually every sector and industry. As AI systems continue to advance in their capabilities, economists and policymakers are grappling with complex questions about how these technologies will reshape labor markets, productivity, economic inequality, and the very nature of economic growth. The potential productivity gains from AI implementation are substantial and multifaceted. By automating routine cognitive and physical tasks, AI can significantly reduce operational costs while simultaneously improving output quality and consistency. Machine learning algorithms can identify patterns and optimize processes in ways that surpass human analytical capabilities, potentially leading to breakthroughs in fields ranging from drug discovery to materials science. Natural language processing and computer vision technologies are enabling the automation of tasks previously thought to require human intelligence, such as document analysis, customer service interactions, and quality control inspections. These efficiency improvements could drive substantial economic growth, with some estimates suggesting that AI could contribute up to $15.7 trillion to the global economy by 2030. However, the labor market impacts of AI adoption present a more complex and potentially concerning picture. Unlike previous waves of automation that primarily affected routine physical tasks, AI technologies are increasingly capable of performing non-routine cognitive tasks that have traditionally been the domain of highly educated workers. This has led to concerns about technological unemployment across a broader spectrum of occupations and skill levels. Economists have identified several potential labor market scenarios as AI continues to develop. The displacement scenario suggests that AI will substitute for human labor across many tasks and occupations, potentially leading to significant job losses and requiring major workforce transitions. The complementarity scenario posits that AI will primarily augment human capabilities rather than replace them, creating new categories of jobs that leverage the partnership between human creativity and machine precision. The reinstatement scenario suggests that while AI may eliminate certain jobs, it will simultaneously create new occupations that we cannot yet envision, much as previous technological revolutions have done. The empirical evidence thus far suggests elements of all three scenarios occurring simultaneously, though the balance between them remains uncertain and context-dependent. The distributional consequences of AI adoption are particularly concerning from an economic policy perspective. If AI technologies disproportionately complement the productivity of already high-skilled, high-income workers while substituting for middle and lower-skilled labor, they could exacerbate existing trends toward wage polarization and economic inequality. Furthermore, the network effects and economies of scale associated with AI systems tend to create winner-take-all dynamics that concentrate economic benefits among a small number of companies and individuals. These tendencies toward concentration raise important questions about market power, competition policy, and the appropriate regulatory framework for AI development and deployment. From a macroeconomic perspective, the potential for AI to drive productivity growth comes at a time when many advanced economies have experienced secular stagnation and declining productivity growth rates. If AI can reverse these trends, it could help address fiscal challenges associated with aging populations and high debt levels. However, realizing these benefits requires complementary investments in infrastructure, education, and workforce development. Countries and regions that fail to make these investments risk falling behind in AI adoption and implementation, potentially leading to a new dimension of global economic divergence.\"\n}, {\n  id: 3,\n  title: \"The Evolution of Quantum Computing\",\n  text: \"Quantum computing represents a paradigm shift in computational technology, harnessing the peculiar behaviors of quantum mechanics to perform calculations in fundamentally different ways than classical computers. Unlike classical bits, which exist in a state of either 0 or 1, quantum bits or qubits can exist in a superposition of both states simultaneously, exponentially increasing the information density and processing potential. This quantum parallelism, combined with the phenomena of entanglement and quantum tunneling, theoretically allows quantum computers to solve certain problems exponentially faster than their classical counterparts. The conceptual foundations of quantum computing trace back to the early 1980s, when physicist Richard Feynman proposed using quantum systems to simulate other quantum systems, a task that proved extraordinarily difficult for classical computers. This insight led to the recognition that quantum computers might offer computational advantages for certain problems. Theoretical progress accelerated in the 1990s with the development of Shor's algorithm, which demonstrated that a quantum computer could efficiently factor large integers, potentially undermining widely used public-key cryptography systems that rely on the computational difficulty of this task. Grover's algorithm, also developed during this period, showed that quantum computers could achieve quadratic speedups in unstructured search problems. These theoretical advances stimulated interest in building practical quantum computing devices, leading to experimental milestones such as the creation of the first two-qubit quantum computer in 1998. Early quantum computing hardware primarily utilized superconducting circuits, trapped ions, or nuclear magnetic resonance techniques to implement qubits. Each approach offered different advantages and challenges regarding qubit stability, coherence times, and scalability. The field experienced significant advancement in the mid-2010s as major technology companies and specialized startups invested heavily in quantum computing research and development. This period saw the introduction of the first commercially available quantum computers, albeit with limited capabilities, and the establishment of cloud-based quantum computing services that provided researchers and developers access to quantum processors remotely. Despite these advances, quantum computers face substantial technical challenges that have thus far limited their practical applications. Quantum decoherence, the tendency of quantum systems to lose their quantum properties when interacting with the environment, remains a significant obstacle. Error rates in quantum gates continue to exceed the thresholds required for fault-tolerant quantum computation, necessitating complex error correction techniques that consume substantial qubit resources. Current quantum computers exist in what is sometimes called the noisy intermediate-scale quantum (NISQ) era, characterized by processors with dozens to hundreds of qubits with significant error rates. These devices cannot yet implement full error correction and have limited practical advantages over classical computers for most problems. Nevertheless, researchers are actively exploring potential near-term applications in fields such as quantum chemistry, materials science, and optimization problems where approximate solutions or quantum-classical hybrid approaches may provide advantages. Looking forward, the roadmap to fault-tolerant quantum computing requires continuous improvements in qubit coherence times, gate fidelities, and scalable architectures. Alternative approaches such as topological quantum computing, which uses exotic quasiparticles called anyons to encode information in ways that are inherently protected from certain types of errors, represent promising but still speculative paths forward. Quantum error correction techniques like surface codes will likely play a crucial role in building large-scale quantum computers, though they require significant qubit overhead. Concurrently, algorithm development continues to expand our understanding of potential quantum advantages across diverse domains, from machine learning to cryptography.\"\n}];","map":{"version":3,"names":["mediumTexts","id","title","text"],"sources":["/Users/ericwang/Theseus/typing-racer/src/data/mediumTexts.ts"],"sourcesContent":["export const mediumTexts = [\n  {\n    id: 1,\n    title: \"The Neuroplasticity of the Human Brain\",\n    text: \"Neuroplasticity refers to the brain's remarkable ability to reorganize itself by forming new neural connections throughout life. This phenomenon allows the neurons in the brain to compensate for injury and disease and to adjust their activities in response to new situations or changes in their environment. It was once believed that the brain's neural networks were largely fixed by early adulthood, with minimal capacity for adaptation or change. Modern neuroscientific research has completely upended this perspective, demonstrating that the brain maintains an extraordinary degree of malleability even into advanced age. Neuroplasticity operates on multiple levels, from microscopic changes in individual neurons to large-scale cortical remapping. The term encompasses both structural plasticity, which refers to changes in the physical structure of the brain, and functional plasticity, which refers to the brain's ability to move functions from damaged areas to undamaged ones. At the cellular level, neuroplasticity manifests through mechanisms such as axonal sprouting, dendritic remodeling, and synaptogenesis. Axonal sprouting occurs when undamaged axons grow new nerve endings to reconnect with neurons that have lost their connections. Dendritic remodeling involves changes in the number and distribution of dendritic spines, which are small protrusions that receive input from a single synapse of an axon. Synaptogenesis refers to the formation of new synaptic connections between neurons, facilitated by the production of various neurotrophic factors. These microscopic changes collectively enhance the brain's ability to process and encode information, supporting learning, memory, and recovery from injury. Research has identified several key drivers of neuroplastic change. Environmental enrichment, characterized by novel, complex, and challenging experiences, has been shown to significantly enhance neural development and plasticity. Physical exercise promotes the release of growth factors such as brain-derived neurotrophic factor (BDNF), which supports neuronal survival and growth. Cognitive stimulation through learning new skills or solving complex problems activates neuroplastic mechanisms, strengthening existing neural pathways and creating new ones. Conversely, stress, social isolation, and certain neurotoxins can inhibit neuroplasticity, highlighting the importance of psychosocial factors in brain health. Neuroplasticity has profound implications for rehabilitation following brain injury or stroke. Constraint-induced movement therapy, for example, forces the use of affected limbs by restraining unaffected ones, thereby promoting cortical reorganization and functional recovery. Similarly, cognitive rehabilitation strategies leverage neuroplastic mechanisms to help individuals recover from traumatic brain injuries or neurodegenerative conditions. Some therapeutic approaches aim to harness neuroplasticity through neurofeedback, where individuals learn to modulate their brain activity through real-time feedback, or through brain stimulation techniques that directly influence neural activity. The field of neural engineering has made significant advances in developing brain-computer interfaces that can potentially restore sensory and motor functions by establishing direct communication pathways between the brain and external devices. These technologies rely on the brain's neuroplastic capacity to interpret and adapt to novel inputs. Understanding neuroplasticity has also revolutionized educational approaches. Recognition of sensitive periods in development, during which the brain is particularly receptive to specific types of stimulation, has informed early childhood education strategies. Moreover, the knowledge that learning continues throughout life has validated the importance of lifelong education and cognitive engagement for maintaining brain health into old age. Despite these advances, many questions about neuroplasticity remain unanswered. Researchers continue to investigate the molecular mechanisms that regulate different forms of plasticity, seeking to develop pharmacological interventions that could enhance beneficial neuroplastic changes or inhibit maladaptive ones. The interaction between genetic factors and environmental influences in shaping neuroplastic responses represents another important area of ongoing research.\"\n  },\n  {\n    id: 2,\n    title: \"The Economic Implications of Artificial Intelligence\",\n    text: \"Artificial intelligence (AI) represents one of the most transformative technological forces of our time, with far-reaching economic implications that extend across virtually every sector and industry. As AI systems continue to advance in their capabilities, economists and policymakers are grappling with complex questions about how these technologies will reshape labor markets, productivity, economic inequality, and the very nature of economic growth. The potential productivity gains from AI implementation are substantial and multifaceted. By automating routine cognitive and physical tasks, AI can significantly reduce operational costs while simultaneously improving output quality and consistency. Machine learning algorithms can identify patterns and optimize processes in ways that surpass human analytical capabilities, potentially leading to breakthroughs in fields ranging from drug discovery to materials science. Natural language processing and computer vision technologies are enabling the automation of tasks previously thought to require human intelligence, such as document analysis, customer service interactions, and quality control inspections. These efficiency improvements could drive substantial economic growth, with some estimates suggesting that AI could contribute up to $15.7 trillion to the global economy by 2030. However, the labor market impacts of AI adoption present a more complex and potentially concerning picture. Unlike previous waves of automation that primarily affected routine physical tasks, AI technologies are increasingly capable of performing non-routine cognitive tasks that have traditionally been the domain of highly educated workers. This has led to concerns about technological unemployment across a broader spectrum of occupations and skill levels. Economists have identified several potential labor market scenarios as AI continues to develop. The displacement scenario suggests that AI will substitute for human labor across many tasks and occupations, potentially leading to significant job losses and requiring major workforce transitions. The complementarity scenario posits that AI will primarily augment human capabilities rather than replace them, creating new categories of jobs that leverage the partnership between human creativity and machine precision. The reinstatement scenario suggests that while AI may eliminate certain jobs, it will simultaneously create new occupations that we cannot yet envision, much as previous technological revolutions have done. The empirical evidence thus far suggests elements of all three scenarios occurring simultaneously, though the balance between them remains uncertain and context-dependent. The distributional consequences of AI adoption are particularly concerning from an economic policy perspective. If AI technologies disproportionately complement the productivity of already high-skilled, high-income workers while substituting for middle and lower-skilled labor, they could exacerbate existing trends toward wage polarization and economic inequality. Furthermore, the network effects and economies of scale associated with AI systems tend to create winner-take-all dynamics that concentrate economic benefits among a small number of companies and individuals. These tendencies toward concentration raise important questions about market power, competition policy, and the appropriate regulatory framework for AI development and deployment. From a macroeconomic perspective, the potential for AI to drive productivity growth comes at a time when many advanced economies have experienced secular stagnation and declining productivity growth rates. If AI can reverse these trends, it could help address fiscal challenges associated with aging populations and high debt levels. However, realizing these benefits requires complementary investments in infrastructure, education, and workforce development. Countries and regions that fail to make these investments risk falling behind in AI adoption and implementation, potentially leading to a new dimension of global economic divergence.\"\n  },\n  {\n    id: 3,\n    title: \"The Evolution of Quantum Computing\",\n    text: \"Quantum computing represents a paradigm shift in computational technology, harnessing the peculiar behaviors of quantum mechanics to perform calculations in fundamentally different ways than classical computers. Unlike classical bits, which exist in a state of either 0 or 1, quantum bits or qubits can exist in a superposition of both states simultaneously, exponentially increasing the information density and processing potential. This quantum parallelism, combined with the phenomena of entanglement and quantum tunneling, theoretically allows quantum computers to solve certain problems exponentially faster than their classical counterparts. The conceptual foundations of quantum computing trace back to the early 1980s, when physicist Richard Feynman proposed using quantum systems to simulate other quantum systems, a task that proved extraordinarily difficult for classical computers. This insight led to the recognition that quantum computers might offer computational advantages for certain problems. Theoretical progress accelerated in the 1990s with the development of Shor's algorithm, which demonstrated that a quantum computer could efficiently factor large integers, potentially undermining widely used public-key cryptography systems that rely on the computational difficulty of this task. Grover's algorithm, also developed during this period, showed that quantum computers could achieve quadratic speedups in unstructured search problems. These theoretical advances stimulated interest in building practical quantum computing devices, leading to experimental milestones such as the creation of the first two-qubit quantum computer in 1998. Early quantum computing hardware primarily utilized superconducting circuits, trapped ions, or nuclear magnetic resonance techniques to implement qubits. Each approach offered different advantages and challenges regarding qubit stability, coherence times, and scalability. The field experienced significant advancement in the mid-2010s as major technology companies and specialized startups invested heavily in quantum computing research and development. This period saw the introduction of the first commercially available quantum computers, albeit with limited capabilities, and the establishment of cloud-based quantum computing services that provided researchers and developers access to quantum processors remotely. Despite these advances, quantum computers face substantial technical challenges that have thus far limited their practical applications. Quantum decoherence, the tendency of quantum systems to lose their quantum properties when interacting with the environment, remains a significant obstacle. Error rates in quantum gates continue to exceed the thresholds required for fault-tolerant quantum computation, necessitating complex error correction techniques that consume substantial qubit resources. Current quantum computers exist in what is sometimes called the noisy intermediate-scale quantum (NISQ) era, characterized by processors with dozens to hundreds of qubits with significant error rates. These devices cannot yet implement full error correction and have limited practical advantages over classical computers for most problems. Nevertheless, researchers are actively exploring potential near-term applications in fields such as quantum chemistry, materials science, and optimization problems where approximate solutions or quantum-classical hybrid approaches may provide advantages. Looking forward, the roadmap to fault-tolerant quantum computing requires continuous improvements in qubit coherence times, gate fidelities, and scalable architectures. Alternative approaches such as topological quantum computing, which uses exotic quasiparticles called anyons to encode information in ways that are inherently protected from certain types of errors, represent promising but still speculative paths forward. Quantum error correction techniques like surface codes will likely play a crucial role in building large-scale quantum computers, though they require significant qubit overhead. Concurrently, algorithm development continues to expand our understanding of potential quantum advantages across diverse domains, from machine learning to cryptography.\"\n  }\n]; "],"mappings":"AAAA,OAAO,MAAMA,WAAW,GAAG,CACzB;EACEC,EAAE,EAAE,CAAC;EACLC,KAAK,EAAE,wCAAwC;EAC/CC,IAAI,EAAE;AACR,CAAC,EACD;EACEF,EAAE,EAAE,CAAC;EACLC,KAAK,EAAE,sDAAsD;EAC7DC,IAAI,EAAE;AACR,CAAC,EACD;EACEF,EAAE,EAAE,CAAC;EACLC,KAAK,EAAE,oCAAoC;EAC3CC,IAAI,EAAE;AACR,CAAC,CACF","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}